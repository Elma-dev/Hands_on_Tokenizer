{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBsgDy5xKwRtZLG0ujuGE/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elma-dev/Hands_on_Tokenizer/blob/main/build_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5GFf9XDbVaO",
        "outputId": "1c2763a6-ea2e-4d5d-9185-90c4f84bda53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[115, 108, 109, 32, 102, 114, 111, 109, 32, 109, 101, 32, 105, 39, 109, 32, 97, 98, 100, 101, 108, 106, 97, 108, 105, 108, 32, 97, 110, 100, 32, 105, 32, 119, 105, 108, 108, 32, 98, 117, 111, 108, 100, 32, 97, 110, 100, 32, 116, 114, 97, 105, 110, 32, 116, 111, 107, 101, 110, 105, 122, 101, 114, 32, 102, 114, 111, 109, 32, 115, 99, 114, 97, 116, 99, 104, 32, 108, 105, 107, 101, 32, 98, 112, 101, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148]\n",
            "len word:93 , len tokens 114\n",
            "ascii('s') -> 115\n"
          ]
        }
      ],
      "source": [
        "word=\"slm from me i'm abdeljalil and i will buold and train tokenizer from scratch like bpe ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”\"\n",
        "# c -> ascii\n",
        "tokens=list(word.encode(\"utf-8\")) # represent word as a list of byte\n",
        "print(tokens)\n",
        "print(f\"len word:{len(word)} , len tokens {len(tokens)}\")\n",
        "# token means each char has an ascii code\n",
        "print(f\"ascii('s') -> {ord(word[0])}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pair_counters(encode_idxs):\n",
        "  pair_count={}\n",
        "  for i in range(len(encode_idxs)-1):\n",
        "    pair=(encode_idxs[i],encode_idxs[i+1])\n",
        "    pair_count[pair]=pair_count.get(pair,0)+1\n",
        "  return pair_count\n",
        "# test\n",
        "pairs_count=pair_counters(tokens)\n",
        "pairs_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5zXAaL6c1xi",
        "outputId": "9f99d5ca-d501-4acc-8fc4-2d5c3e80bd26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(115, 108): 1,\n",
              " (108, 109): 1,\n",
              " (109, 32): 4,\n",
              " (32, 102): 2,\n",
              " (102, 114): 2,\n",
              " (114, 111): 2,\n",
              " (111, 109): 2,\n",
              " (32, 109): 1,\n",
              " (109, 101): 1,\n",
              " (101, 32): 3,\n",
              " (32, 105): 2,\n",
              " (105, 39): 1,\n",
              " (39, 109): 1,\n",
              " (32, 97): 3,\n",
              " (97, 98): 1,\n",
              " (98, 100): 1,\n",
              " (100, 101): 1,\n",
              " (101, 108): 1,\n",
              " (108, 106): 1,\n",
              " (106, 97): 1,\n",
              " (97, 108): 1,\n",
              " (108, 105): 2,\n",
              " (105, 108): 2,\n",
              " (108, 32): 2,\n",
              " (97, 110): 2,\n",
              " (110, 100): 2,\n",
              " (100, 32): 3,\n",
              " (105, 32): 1,\n",
              " (32, 119): 1,\n",
              " (119, 105): 1,\n",
              " (108, 108): 1,\n",
              " (32, 98): 2,\n",
              " (98, 117): 1,\n",
              " (117, 111): 1,\n",
              " (111, 108): 1,\n",
              " (108, 100): 1,\n",
              " (32, 116): 2,\n",
              " (116, 114): 1,\n",
              " (114, 97): 2,\n",
              " (97, 105): 1,\n",
              " (105, 110): 1,\n",
              " (110, 32): 1,\n",
              " (116, 111): 1,\n",
              " (111, 107): 1,\n",
              " (107, 101): 2,\n",
              " (101, 110): 1,\n",
              " (110, 105): 1,\n",
              " (105, 122): 1,\n",
              " (122, 101): 1,\n",
              " (101, 114): 1,\n",
              " (114, 32): 1,\n",
              " (32, 115): 1,\n",
              " (115, 99): 1,\n",
              " (99, 114): 1,\n",
              " (97, 116): 1,\n",
              " (116, 99): 1,\n",
              " (99, 104): 1,\n",
              " (104, 32): 1,\n",
              " (32, 108): 1,\n",
              " (105, 107): 1,\n",
              " (98, 112): 1,\n",
              " (112, 101): 1,\n",
              " (32, 240): 1,\n",
              " (240, 159): 7,\n",
              " (159, 133): 7,\n",
              " (133, 164): 1,\n",
              " (164, 240): 1,\n",
              " (133, 157): 1,\n",
              " (157, 240): 1,\n",
              " (133, 152): 1,\n",
              " (152, 240): 1,\n",
              " (133, 146): 1,\n",
              " (146, 240): 1,\n",
              " (133, 158): 1,\n",
              " (158, 240): 1,\n",
              " (133, 147): 1,\n",
              " (147, 240): 1,\n",
              " (133, 148): 1}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get common redandent pair\n",
        "max(pairs_count,key=pairs_count.get)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZwS5uNjce9w",
        "outputId": "ccd1c1bc-29a7-46ac-dfd0-7fe6d5fe2051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(240, 159)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this function is to represent pair number as another number (1,2) -> 55\n",
        "# that's will reduce the size of encoder list (compress)\n",
        "def merge(encode_idxs,pair,new_id):\n",
        "  new_encode_idxs=[]\n",
        "  i=0\n",
        "  while i<len(encode_idxs):\n",
        "    if i!=len(encode_idxs)-1 and (encode_idxs[i],encode_idxs[i+1])==pair:\n",
        "      new_encode_idxs.append(new_id)\n",
        "      i+=2\n",
        "    else:\n",
        "      new_encode_idxs.append(encode_idxs[i])\n",
        "      i+=1\n",
        "  return new_encode_idxs\n",
        "# test\n",
        "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))\n",
        "# -----------\n",
        "tokens2=merge(tokens,(240, 159),256)\n",
        "print(f\"\"\"old size of tokenizer {len(tokens)}, new size {len(tokens2)} reduce by {len(tokens)-len(tokens2)} \"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_0C-2FEgqbc",
        "outputId": "a264d465-abc4-48e6-b470-d6e41ca81ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 6, 99, 9, 1]\n",
            "old size of tokenizer 114, new size 107 reduce by 7 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do merge n times\n",
        "n=10\n",
        "new_tokens=tokens\n",
        "merges={}\n",
        "for i in range(n):\n",
        "  pair_count=pair_counters(new_tokens)\n",
        "  top_pair=max(pair_count,key=pair_count.get)\n",
        "  new_tokens=merge(new_tokens,top_pair,256+i)\n",
        "  merges[top_pair]=256+i\n",
        "  print(f\"{top_pair} -> {256+i}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9evHvZDkfhj",
        "outputId": "cb2a6398-ee40-4dd1-da2a-60e4649e0d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 159) -> 256\n",
            "(256, 133) -> 257\n",
            "(109, 32) -> 258\n",
            "(101, 32) -> 259\n",
            "(100, 32) -> 260\n",
            "(102, 114) -> 261\n",
            "(261, 111) -> 262\n",
            "(262, 258) -> 263\n",
            "(108, 105) -> 264\n",
            "(108, 32) -> 265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\"\"old size of tokenizer {len(tokens)}, new size {len(new_tokens)} reduce by {len(tokens)-len(new_tokens)} \"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnWZGnMUm8JU",
        "outputId": "2558dadb-b404-4894-feaa-f51e839cde95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "old size of tokenizer 114, new size 80 reduce by 34 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab={i:bytes([i]) for i in range(256)}\n",
        "for (p0,p1),i in merges.items():\n",
        "  vocab[i]=vocab[p0]+vocab[p1]\n",
        "\n",
        "def decode(ids):\n",
        "  tokens=b\"\".join([vocab[i] for i in ids])\n",
        "  text=tokens.decode(\"utf-8\",errors=\"replace\")\n",
        "  return text\n",
        "decode([128])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "re8xeWeejkPT",
        "outputId": "b23c3b5e-a9ad-463e-80ae-f482bf88f01f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ï¿½'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text):\n",
        "  tokens=list(text.encode(\"utf-8\"))\n",
        "  while len(tokens)>2:\n",
        "    pair_count=pair_counters(tokens)\n",
        "    pair=min(pair_count,key=lambda p: merges.get(p,float(\"inf\")))\n",
        "    if pair not in merges:\n",
        "      break\n",
        "    idx=merges[pair]\n",
        "    tokens=merge(tokens,pair,idx)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "hciBxaNWcDFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "text=\"hello world\"\n",
        "decode(encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-UQVNqHxnm2z",
        "outputId": "7c2650da-2b0e-4e0b-d845-21f055186271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello world'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "class Tokenizer():\n",
        "  def __init__(self,re_pattern):\n",
        "    self.merges={}\n",
        "    self.last_id=255\n",
        "    self.vocab={i:bytes([i]) for i in range(256)}\n",
        "    self.compiled_pattern=re.compile(re_pattern)\n",
        "\n",
        "  def train(self,text,nbr_tokens):\n",
        "    chunks=re.findall(self.compiled_pattern,text)\n",
        "    tokens_ids=[list(map(int,chunk.encode(\"utf-8\")))  for chunk in chunks]\n",
        "    for i in range(nbr_tokens):\n",
        "      pair_count={}\n",
        "      for token_ids in tokens_ids:\n",
        "        self.__pair_counter(token_ids,pair_count)\n",
        "      pair=max(pair_count,key=pair_count.get)\n",
        "      tokens_ids=self.__merge(tokens_ids,pair,self.last_id+1)\n",
        "      self.merges[pair]=self.last_id+1\n",
        "      self.vocab[self.last_id+1]=self.vocab[pair[0]]+self.vocab[pair[1]]\n",
        "      self.last_id+=1\n",
        "    return tokens_ids\n",
        "\n",
        "  def __pair_counter(self,tokens_ids,pair_count):\n",
        "    for i in range(len(tokens_ids)-1):\n",
        "      pair=(tokens_ids[i],tokens_ids[i+1])\n",
        "      pair_count[pair]=pair_count.get(pair,0)+1\n",
        "    return pair_count\n",
        "\n",
        "  def __merge(self,tokens_ids,pair,new_id):\n",
        "    new_encode_idxs=[]\n",
        "    i=0\n",
        "    while i<len(tokens_ids):\n",
        "      if i!=len(tokens_ids)-1 and (tokens_ids[i],tokens_ids[i+1])==pair:\n",
        "        new_encode_idxs.append(new_id)\n",
        "        i+=2\n",
        "      else:\n",
        "        new_encode_idxs.append(tokens_ids[i])\n",
        "        i+=1\n",
        "    return new_encode_idxs\n",
        "\n",
        "  def encode(self,text):\n",
        "    chunks=re.findall(self.compiled_pattern,text)\n",
        "    results=[]\n",
        "    for text in chunks:\n",
        "      tokens=list(text.encode(\"utf-8\"))\n",
        "      while len(tokens)>2:\n",
        "        pair_count={}\n",
        "        self.__pair_counter(tokens,pair_count)\n",
        "        pair=min(pair_count,key=lambda p: self.merges.get(p,float(\"inf\")))\n",
        "        if pair not in self.merges:\n",
        "          break\n",
        "        idx=self.merges[pair]\n",
        "        tokens=self.__merge(tokens,pair,idx)\n",
        "      results.extend(tokens)\n",
        "    return results\n",
        "\n",
        "  def decode(self,ids):\n",
        "    tokens=b\"\".join([self.vocab[i] for i in ids])\n",
        "    text=tokens.decode(\"utf-8\",errors=\"replace\")\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "C2ccsDrujSzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        "valtext = \"Many common characters, including numerals, punctuation, and other symbols, are unified within the standard and are not treated as specific to any given writing system. Unicode encodes thousands of emoji, with the continued development thereof conducted by the Consortium as a part of the standard.[4] Moreover, the widespread adoption of Unicode was in large part responsible for the initial popularization of emoji outside of Japan. Unicode is ultimately capable of encoding more than 1.1 million characters.\"\n",
        "tokenizer=Tokenizer(DARIJA_SPLIT_PATTERN)"
      ],
      "metadata": {
        "id": "lC6B_bVOIE-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train(valtext,30)\n",
        "\"nti hya lmerda\"==tokenizer.decode(tokenizer.encode(\"nti hya lmerda\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "vkJQkr6GBaSn",
        "outputId": "3945670a-4f73-4f13-b21f-0f71965b8b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "max() arg is an empty sequence",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-b7023f85fe85>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvaltext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\"nti hya lmerda\"\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nti hya lmerda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-0a176ea37e3f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, text, nbr_tokens)\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mtoken_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pair_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpair_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mpair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpair_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0mtokens_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"nti hya lmerda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zntkwzHzK1tp",
        "outputId": "2a04bbd3-28d3-47d7-e149-a340bb092610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[110, 116, 105, 32, 104, 121, 97, 32, 108, 109, 101, 114, 100, 97]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DARIJA_SPLIT_PATTERN = r\"\"\"\n",
        "    \\'(?:[Ù„Ø¨]|Ù‡[Ø§Ùˆ]|Ù‡Ù…|Ù‡Ø§|Ùƒ|Ù†ÙŠ)|\n",
        "    ?[\\p{Arabic}\\p{Latin}]+|\n",
        "    ?[\\p{N}Ù -Ù©]+|\n",
        "    ?[^\\s\\p{Arabic}\\p{Latin}\\p{N}Ù -Ù©]+|\n",
        "    \\s+(?!\\S)|\n",
        "    \\s+ # Whitespace\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kI3_S2uaIzcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fBPQ2KsIMkAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ytt_6o8EIf_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}